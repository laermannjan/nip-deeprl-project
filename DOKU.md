 # Brainstorming  
 ## Problem with diverging after many episode (thoughts so far):  
 Replaybuffer is filled only with good samples, and then receives a bad episode(e.g. due to 10% final exploration), the Q-function updates heavily into the wrong direction
 and often the performance gets alot worse before partly recovering. This behavior is was also stated in LeBruin16, who proposed a sampling strategy to solve this issue. Here, they sample from a 'bad' experience replaybuffer and a 'good' one, to keep enough bad samples to learn from in memory. In our tests we activated the already implemented prioritized sampling and are planning to compare this approach to standard sampling.  
These findings are supported by the saved videos over the course of learning. LunarLander, as well as Acrobot. While a near optimal policy is often found after certain amount of episodes (Acrobot hast smooth motion and short episodes, LunarLander is approaching the ground fast), the learned progress is often overwritten and Acrobot and LunarLander take a longer, less rewarded approach to solve the games. While still performing well, the behavior is changed a lot (from fast deterministic accelerating the limbs to a more chaotic and time consuming actions in Acrobot) (from near falling and stop shortly above the goal to 'falling leaf-like' motion, which seems less risky but performes a bit worse in terms of total reward but more consistent with regard to average reward. This may be due to the lower limit of our exploration rate. Esepcially with the LunarLander. Theoretical thought: When the LunarLander lerns a fast solution, where it kind of falls to the ground and still hast e.g. 10% random action, those action may lead to crashes with very high negative reward, compared to high postive reward, for nearly the same actions. The propagated Q-function update may destroy previously learned trajectories.  


## Intuition about number of neurons and layer (thoughts so far):  
With to few neurons, the learning fails. This seems kidna straight forward, since the capacity of the DQN is not high enough to learn meaningfull tactics. While this may be less important with the Acrobot (here it seems that too many neurons also weakens the performance, more later), the LunarLander learns artefacts of the policy, e.g. just hovers around the goal instead of landing (e.g. 30-30-30). This doesn't happen with enough neurons (e.g. 150-150-150), where a fast solution is usually found.  
A maybe similar beavior can be seen with the Acrobot, where too few neurons (10-10-10) just oscilate without moving the limbs, and too many neurons (512-256-128) converge to slower solutions (~700 steps/episode average in the end - often with faster solutions earlier from which they diverge) while a small network (100-80) consistently found fast solutions around 200 steps/episode and didn't diverge again.  
The verdict about the effect of neuron arrangement is still out, but decreasing number of nodes seem to converge faster than equally spread neurons. We're running experients with (128-128-128) (128-192-256) (256-192-128) layer and different exploration rates (final e: 10%/1%) and shorter and longer annealing, for Acrobot-v1 and Lunarlander-v2.


